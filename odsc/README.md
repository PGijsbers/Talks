# Tutorial on Automated Machine Learning at ODSC London (2019-11-20)

**Abstract**: 
Automated machine learning is the science of building machine learning models in a data-driven, efficient, and objective way. 
It replaces manual trial-and-error with automated, guided processes. 
In this tutorial, we will guide you through the current state of the art in hyperparameter optimization, pipeline construction, and neural architecture search. 
We will discuss model-free blackbox optimization methods, Bayesian optimization, as well as evolutionary and other techniques. 
We will also pay attention to meta-learning, i.e. learning how to build machine learning models based on prior experience. 
Moreover, we will give practical guidance on how to do meta-learning with OpenML, an online platform for sharing and reusing machine learning experiments, 
and how to perform automated pipeline construction with GAMA, a novel, research-oriented AutoML tool in Python.

----
# Session Overview

The first part of the tutorial is given by @joaquinvanschoren.
He talks about the current state of the art in AutoML and meta-learning.
In the second part I (@PGijsbers) will talk about using OpenML for meta-learning, and GAMA for AutoML.
This repository contains the reference materials for the second part of the session.

## Using OpenML
[OpenML](www.openml.org) is an open platform for open science collaboration in machine learning,
used to share datasets and results of machine learning experiments.
It defines machine learning experiments through the following concepts:

 - *Datasets*. The regular (tabular) datasets. 
 - *Tasks*. Define splits of a dataset that should be used to evaluate a model (in a reproducible way). E.g. 10-fold cross-validation.
 - *Flows*. Define algorithms, workflows or scripts solving tasks. E.g. RandomForest, auto-sklearn.
 - *Runs*. The result of running a Flow on a Task, i.e. a recorded machine learning experiment.
   Concrete it is a set of predictions generated by the trained model, corresponding score, and some meta-data on its execution.

OpenML hosts thousands of datasets and flows and has millions of runs (recorded machine learning experiment).
All this information is publicly available for downloading through any of its interfaces (website, API's in R, Java, Python and C#).
This data can be used to answer many questions, and in this tutorial sessions we will explore two settings.

First, we will visualize the effect of tuning the hyperparameters 'C' and 'Gamma' of a Support Vector Machine for a specific problem.
Secondly use the (public) results of machine learning experiments to train a surrogate model, 
which will predict the performance of an algorithm for a given problem.

### Visualize Hyperparameter Surfaces
An adopted version of [this example](https://openml.github.io/openml-python/master/examples/30_extended/plot_svm_hyperparameters_tutorial.html#sphx-glr-examples-30-extended-plot-svm-hyperparameters-tutorial-py).

### Training a Surrogate Model
An adopted version of [this example](https://openml.github.io/openml-python/master/examples/40_paper/2018_neurips_perrone_example.html#sphx-glr-examples-40-paper-2018-neurips-perrone-example-py)

## GAMA
Automated Machine Learning 

### Architecture - Basics
At the most basic level, you use GAMA like it is a scikit-learn estimator (this is also true for e.g. auto-sklearn and TPOT).
That means you call a `fit` function with your data, starts the search for a good model and trains it.
After model training is complete, the `predict` function can be called to produce predictions for new data.

Using 

<!-- As a little bonus, here's how to work with auto-sklearn and TPOT: -->

### Architecture - Advanced
But GAMA allows more control over your AutoML system.
We're working on a configurable pipeline, at this time you can:
 - Pick a model search algorithm
 - Pick a post-processing method

### Optimization
There are multiple ways to search for good machine learning pipelines.
Popular methods include successive halving (Hyperband), Bayesion optimization (auto-sklearn) and evolutionary optimization (TPOT).
GAMA currently features implementations of successive halving and evolutionary optimization.

#### Asynchronous Successive Halving

#### Asynchronous Evolution

#### Multi-Objective Optimization with NSGA-II

### Other ways to use GAMA
CLI/User Interface.
