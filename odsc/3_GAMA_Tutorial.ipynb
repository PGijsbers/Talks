{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GAMA for Automated Machine Learning\n",
    "This notebook is used at the Tutorial on Automated Machine Learning at ODSC Europe 2019.\n",
    "It shows how to use GAMA for automated machine learning.\n",
    "Most of the information in this notebook can also be found in the [documentation](https://pgijsbers.github.io/gama/).\n",
    "If you are reading this notebook at a later time, and find its outdated, be sure to check out the documentation instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAMA is hosted on PyPI, so installing it is easy:\n",
    "```bash\n",
    "> pip install gama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GAMA in Python\n",
    "First we need to load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x_train: (13400, 16), x_test: (6600, 16), y_train: (13400,), y_test: (6600,)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openml\n",
    "letter_holdout = openml.tasks.get_task(236)\n",
    "train, test = letter_holdout.get_train_test_split_indices()\n",
    "x, y = letter_holdout.get_X_and_y()\n",
    "x_train, x_test, y_train, y_test = x[train, :], x[test, :], y[train], y[test]\n",
    "f'x_train: {x_train.shape}, x_test: {x_test.shape}, y_train: {y_train.shape}, y_test: {y_test.shape}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for AutoML.\n",
    "Using GAMA through its Python API works much like you would a scikit-learn estimator.\n",
    "We must first initialize either a `GamaClassifier` or `GamaRegressor` and specify its hyperparameters.\n",
    "For now, we keep it simple and only specify how long we want GAMA to optimize for, and what to communicate back to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GAMA version 19.11.4.\n",
      "GamaClassifier(post_processing_method=BestFitPostProcessing(),search_method=AsyncEA(),cache_dir=None,keep_analysis_log=D:\\repositories\\Talks\\odsc\\gama.log,verbosity=20,n_jobs=None,max_eval_time=None,max_total_time=180,random_state=None,regularize_length=True,scoring=accuracy)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gama import GamaClassifier\n",
    "\n",
    "automl = GamaClassifier(\n",
    "    max_total_time=180,  # GAMA can use at most 180 seconds for fit.\n",
    "    verbosity=logging.INFO,  # We want to see INFO messages.\n",
    "    scoring='accuracy'\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `fit`, GAMA will start searching for a good machine learning pipeline for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing took 0.0290s.\n",
      "Starting EA with new population.\n",
      "Current Pareto front updated: RobustScaler>VarianceThreshold>LinearSVC scores (-inf, -3)\n",
      "Current Pareto front updated: BernoulliNB scores (0.1132089552238806, -1)\n",
      "Current Pareto front updated: PCA>GaussianNB scores (0.6905970149253732, -2)\n",
      "Current Pareto front updated: ExtraTreesClassifier scores (0.9501492537313433, -1)\n",
      "Current Pareto front updated: ExtraTreesClassifier scores (0.9556716417910448, -1)\n",
      "Search phase evaluated 135 individuals.\n",
      "search took 161.0380s.\n",
      "postprocess took 2.1004s.\n",
      "Attempting to delete 20191119_152844_e83a_GAMA\n"
     ]
    }
   ],
   "source": [
    "automl.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 23,  7, 24, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.predict(x_test[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy: 0.9577272727272728'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{automl._metrics[0].name}: {automl.score(x_test, y_test)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                missing_values=nan, strategy='median', verbose=0)),\n",
       " ('0',\n",
       "  ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=0.6000000000000001,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=3,\n",
       "                       min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                       random_state=None, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl.model.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note:* `TPOT` and `auto-sklearn` follow the same scikit-learn convention of `fit` and `predict`, so now you know how to use them too! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So what's going on?\n",
    "\n",
    "In this section, we shine some light on *how* GAMA performs automated machine learning.\n",
    "By default GAMA uses an evolutionary algorithm to optimize pipelines, we break it down in pseudo-code.\n",
    "\n",
    "There are some differences between what is described below and what is implemented in GAMA. That said, the code below conveys the general strategy of using evolution to guide search, and is not off too far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def optimize():\n",
    "    # Create an initial random population\n",
    "    population = [new() for _ in range(20)]\n",
    "    for pipeline in population:\n",
    "        score(pipeline)\n",
    "        \n",
    "    while there is time left:\n",
    "        new_pipeline = create_from_population(population)\n",
    "        population.remove(worst_pipeline)  # worst pipeline determined by its assigned scores\n",
    "        \n",
    "        score(new_pipeline)\n",
    "        population = population + [new_pipeline]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Pipelines\n",
    "\n",
    "In GAMA, we use scikit-learn base classes to determine if an algorithm is a preprocessing algorithm (`Transformer`) or an algorithm which builds a predictive model (`Estimator`). We can randomly sample an `Estimator` and add any number (or zero) of `Transformer` components for preprocessing to make a new pipeline. Which algorithms to consider, and what their hyperparameters are, is defined in a configuration file (not discussed in the tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def new() -> Pipeline:  # Creates new pipeline at random.\n",
    "```\n",
    "\n",
    "We define a `mutate` function to make changes to a single pipeline:\n",
    "```python\n",
    "def mutate(pipeline) -> Pipeline: \n",
    "    # Create a variation of the given pipeline: \n",
    "    # - Add or remove a step\n",
    "    # - Change the hyperparameter configuration of a step\n",
    "    # - Replace a step (e.g. change the estimator)\n",
    "```\n",
    "And a `crossover` function to combine two pipelines into a new one:\n",
    "```python\n",
    "def crossover(pipeline, other_pipeline) -> Pipeline:\n",
    "    # Creates a new pipeline by combining two others:\n",
    "    # - Swap (parts of) the preprocessing pipeline\n",
    "    # - Swap (some of) the hyperparameters of a shared step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create new pipelines from the population by randomly mutating a single pipeline, or performing crossover on two pipelines:\n",
    "```python\n",
    "def create_from_population(pipelines):\n",
    "    if ...:  # Select with some probability\n",
    "        pipeline = select(pipelines, n=1)\n",
    "        return mutate(pipeline)\n",
    "    else:\n",
    "        pl1, pl2 = select(pipelines, n=2)\n",
    "        return crossover(pl1, pl2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Pipelines\n",
    "\n",
    "Selects pipelines that are 'good' for creating offspring.\n",
    "It can be any kind of selection procedure which takes into account the scores of the pipelines.\n",
    "For example (NSGA-II):\n",
    " 1. Select two pipelines at random\n",
    " 2. \n",
    "     - If they are not on the same pareto front, pick the one on the best front.\n",
    "     - If they are on the same pareto front, decide based on 'crowding distance'.\n",
    " 3. Repeat 1 & 2 n times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"height:400px; margin-right: 30%\" src=\"https://raw.githubusercontent.com/PGijsbers/Talks/master/odsc/images/gama/pareto.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the AutoML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we want you to specify 'resource' and 'preference' hyperparameters, you can also specify hyperparameters which influence the AutoML process. In GAMA you can pick which optimization algorithm to use, or to create an ensemble of pipelines after search. Each of these steps also have their own hyperparameters which may be specified! But don't worry - we do our best to find good defaults so we can keep AutoML automatic. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gama.search_methods import AsynchronousSuccessiveHalving\n",
    "from gama.postprocessing import EnsemblePostProcessing \n",
    "automl = GamaClassifier(\n",
    "    search_method=AsynchronousSuccessiveHalving(),\n",
    "    post_processing_method=EnsemblePostProcessing()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our look at `gama`. \n",
    "If you're interested in learning more about the package, visit the [documentation](https://pgijsbers.github.io/gama/).\n",
    "If you need help, find bugs, have feature requests or want to get in touch visit our Github repository at [PGijsbers/gama](https://github.com/PGijsbers/gama). User feedback is always appreciated!\n",
    "\n",
    "In a short look ahead, very soon we will also release a first version of our Command Line Interface and Graphical User Interface!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra on Mutation and Crossover\n",
    "Code below illustrates the creation of pipelines in GAMA, in particular the mutation and crossover of pipelines.\n",
    "First we create two pipelines to modify. Ignore most of the code, what's important is that we create two pipelines here.\n",
    "One (`ss_bnb`) transforms the data with **S**tandard**S**caler and creates a model with **B**ernoulli**NB**, the second (`mm_dt`) transforms the data with a **M**in**M**axScaler and creates a model with a **D**ecision**T**ree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(StandardScaler(data), alpha=1.0, fit_prior=True)\n",
      "DecisionTreeClassifier(MinMaxScaler(data), DecisionTreeClassifier.criterion='gini', DecisionTreeClassifier.max_depth=3, min_samples_leaf=5, min_samples_split=10)\n"
     ]
    }
   ],
   "source": [
    "from gama.genetic_programming.components import Individual\n",
    "\n",
    "ss_bnb = Individual.from_string('BernoulliNB(StandardScaler(data), alpha=1.0, fit_prior=True)', automl._pset)\n",
    "print(ss_bnb.pipeline_str())\n",
    "\n",
    "mm_dt = Individual.from_string(\n",
    "    \"DecisionTreeClassifier(\"\n",
    "        \"MinMaxScaler(data), \"\n",
    "        \"DecisionTreeClassifier.criterion='gini', \"  # Some hyperparameters here are prefixed,\n",
    "        \"DecisionTreeClassifier.max_depth=3, \"       # you can ignore this in the tutorial.\n",
    "        \"min_samples_split=10, \"\n",
    "        \"min_samples_leaf=5)\",\n",
    "    automl._pset\n",
    ")\n",
    "print(mm_dt.pipeline_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure below you see the different mutations that are in use by GAMA.\n",
    "A machine learning pipeline is represented by a set of shapes (circles or squares) connected by arrows.\n",
    "Each color represents a different algorithm, two different shapes with the same color represent the same algorithm but a different hyperparameter configuration.\n",
    "\n",
    "On the left you see the pipelines before mutation, and on the right you see the pipeline after the mutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"height:400px; margin-right: 15%\" src=\"https://raw.githubusercontent.com/PGijsbers/Talks/master/odsc/images/gama/mutations_txt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four different mutations are:\n",
    " - Removing a step (first line).\n",
    " - Replacing a step (second line).\n",
    " - Changing the hyperparameter configuration of a step (third line).\n",
    " - Adding a step (fourth line).\n",
    " \n",
    "Call the code block below to get some mutated versions of our `ss_bnb` pipeline!\n",
    "You might need to try a few times, but you should be able to find all four mutations can be applied to this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: BernoulliNB(StandardScaler(data), alpha=1.0, fit_prior=True)\n",
      "After: BernoulliNB(data, alpha=1.0, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "mutated = automl._operator_set.mutate(ss_bnb)\n",
    "print(f'Before: {ss_bnb.pipeline_str()}\\nAfter: {mutated.pipeline_str()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next figure, we visualize crossover in the same way. There are two ways crossover is applied in GAMA:\n",
    " - Swapping (part of) the preprocessing pipelines (first line).\n",
    " - Changing (a subset of) hyperparameter values when the pipelines share a step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"height:400px; margin-right: 15%\" src=\"https://raw.githubusercontent.com/PGijsbers/Talks/master/odsc/images/gama/crossover_txt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below performs crossover between the `ss_bnb` and `mm_dt` pipelines. As they have no steps in common, they can't swap hyperparameter configurations. As such, the only crossover method employed is swapping preprocessing pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parents:\n",
      " StandardScaler>BernoulliNB\n",
      " MinMaxScaler>DecisionTreeClassifier\n",
      "Created:\n",
      " MinMaxScaler>BernoulliNB\n"
     ]
    }
   ],
   "source": [
    "mated = automl._operator_set.mate(ss_bnb, mm_dt)\n",
    "print(f'Parents:\\n {ss_bnb.short_name}\\n {mm_dt.short_name}\\nCreated:')\n",
    "print(f' {mated.short_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
